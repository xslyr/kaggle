{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30579,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef corr_heatmap( df, cols=None, figsize=(14,7)):\n    fig, ax = plt.subplots(figsize=figsize)\n    corr = df.loc[:,cols].corr() if cols!=None else df.corr()\n    sns.heatmap( corr , annot=True, ax=ax)\n    plt.show()\n    \nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ntrain = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest = pd.read_csv('/kaggle/input/titanic/test.csv')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-23T16:52:04.287228Z","iopub.execute_input":"2023-11-23T16:52:04.287617Z","iopub.status.idle":"2023-11-23T16:52:05.283113Z","shell.execute_reply.started":"2023-11-23T16:52:04.287570Z","shell.execute_reply":"2023-11-23T16:52:05.281995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('train.csv have PassengerIds between {} and {}'.format( train.PassengerId[0], train.PassengerId.values[-1] ))\nprint('test.csv (for submission) have PassengerIds between {} and {}'.format( test.PassengerId[0], test.PassengerId.values[-1] ))\n\ny_data = train['Survived'].copy() #train.pop('Survived')\nx_data = pd.concat([train,test], axis=0)\nx_data = x_data.reset_index(drop=True)\n\nprint('\\nThe data that we\\'ll use for train is a merge between train/test. So the shape of our x_data is {}.'.format(x_data.shape))","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:52:05.284924Z","iopub.execute_input":"2023-11-23T16:52:05.285251Z","iopub.status.idle":"2023-11-23T16:52:05.312242Z","shell.execute_reply.started":"2023-11-23T16:52:05.285222Z","shell.execute_reply":"2023-11-23T16:52:05.311025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ***Data Analysis and Treatment:***","metadata":{}},{"cell_type":"code","source":"x_data.info()","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:52:05.313853Z","iopub.execute_input":"2023-11-23T16:52:05.314436Z","iopub.status.idle":"2023-11-23T16:52:05.347605Z","shell.execute_reply.started":"2023-11-23T16:52:05.314401Z","shell.execute_reply":"2023-11-23T16:52:05.346519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_data = x_data.drop(['PassengerId'], axis=1)\n\n# The pclass column is related to \"Ticket class\", so its necessary to treated as categorical.\nx_data.Pclass = x_data.Pclass.astype(object)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:52:05.349059Z","iopub.execute_input":"2023-11-23T16:52:05.350074Z","iopub.status.idle":"2023-11-23T16:52:05.357988Z","shell.execute_reply.started":"2023-11-23T16:52:05.350035Z","shell.execute_reply":"2023-11-23T16:52:05.356801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nulls = x_data.isna().sum().to_dict()\n\nfor col in nulls:\n    if nulls[col]>0:\n        print(f\"{nulls[col]}\\t{x_data[col].dtype}\\t\\t{col}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:52:05.361576Z","iopub.execute_input":"2023-11-23T16:52:05.362193Z","iopub.status.idle":"2023-11-23T16:52:05.372230Z","shell.execute_reply.started":"2023-11-23T16:52:05.362158Z","shell.execute_reply":"2023-11-23T16:52:05.371470Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for 'Fare' and 'Age' columns we'll change nan by mean values \nx_data.Age = x_data.Age.fillna(x_data.Age.mean())\nx_data.Fare = x_data.Fare.fillna(x_data.Fare.mean())\n\n# on 'Embarked' column we'll use the most repeated value(mode)\nx_data.Embarked.fillna( 'UNK', inplace=True)\n\n# for null values on Cabin column, the label \"None\" will be adopted\nx_data.Cabin.fillna('_', inplace=True)\nx_data.isna().sum().to_dict()","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:52:05.373636Z","iopub.execute_input":"2023-11-23T16:52:05.374104Z","iopub.status.idle":"2023-11-23T16:52:05.393232Z","shell.execute_reply.started":"2023-11-23T16:52:05.374075Z","shell.execute_reply":"2023-11-23T16:52:05.392186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ***Feature Engineering:***","metadata":{}},{"cell_type":"code","source":"x_data['Individual_Fare'] = np.log1p(x_data.Fare/(1+ x_data.SibSp + x_data.Parch ))","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:52:05.394948Z","iopub.execute_input":"2023-11-23T16:52:05.395601Z","iopub.status.idle":"2023-11-23T16:52:05.402766Z","shell.execute_reply.started":"2023-11-23T16:52:05.395564Z","shell.execute_reply":"2023-11-23T16:52:05.401939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\nimport statistics as stats\n\ndef clean_textname(field:pd.Series):\n    chars = [',','.',',','(',')','-','\"','Miss','Mrs','Mr','Master']\n    for c in chars: field = field.apply(lambda x: str(x).replace(c,''))\n    return field\n    \ndef calc_probabilities(dataframe: pd.DataFrame, survived:int):\n    selected_names = dataframe.loc[ dataframe.Survived==survived ].Name\n    names = []\n    for item in selected_names.values:\n        word = item.split(' ')\n        try: \n            word.remove('')\n            word.remove(' ')\n        except: pass\n        names += word\n        \n    names = dict(zip( Counter(names).keys(),Counter(names).values() )) \n    probs = pd.DataFrame(names, index=[str(survived)]).T\n    count = probs[str(survived)].count()\n    return probs.apply(lambda x: x/count) if survived == 1 else probs.apply(lambda x: -1*x/count)\n \n\nx_data.Name = clean_textname(x_data.Name)\n\nname_probability = pd.concat([calc_probabilities(x_data,0),calc_probabilities(x_data,1)], axis=1).fillna(0)\nname_probability['score'] = name_probability.apply(lambda x: 100*x.sum(), axis=1)\nname_probability = name_probability.score.to_dict()\n\nname_score = []\nfor name in x_data.Name:\n    \n    try: _max = max([ name_probability[word] if len(word)>0 else -np.inf for word in name.split(' ') ])\n    except: _max = 0\n    \n    try: _mean = stats.mean([ name_probability[word] if len(word)>0 else 0 for word in name.split(' ') ])\n    except: _mean = 0\n    \n    try: _sum = sum([ name_probability[word] if len(word)>0 else 0 for word in name.split(' ') ])\n    except: _sum = 0\n        \n    name_score.append([_max, _mean, _sum])\n\nnew_columns = pd.DataFrame( name_score , columns=['Name_Score_Max','Name_Score_Mean','Name_Score_Sum'])\nx_data = pd.concat([x_data, new_columns], axis=1)\n\nx_data = x_data.drop(['Name', 'Ticket','Cabin'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:52:05.404089Z","iopub.execute_input":"2023-11-23T16:52:05.404726Z","iopub.status.idle":"2023-11-23T16:52:05.671729Z","shell.execute_reply.started":"2023-11-23T16:52:05.404688Z","shell.execute_reply":"2023-11-23T16:52:05.670725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_heatmap( pd.get_dummies(x_data) ,figsize=(20,10))","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:52:05.673129Z","iopub.execute_input":"2023-11-23T16:52:05.673587Z","iopub.status.idle":"2023-11-23T16:52:07.077896Z","shell.execute_reply.started":"2023-11-23T16:52:05.673517Z","shell.execute_reply":"2023-11-23T16:52:07.076711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_data.hist(figsize=(20,10))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:52:07.079094Z","iopub.execute_input":"2023-11-23T16:52:07.079741Z","iopub.status.idle":"2023-11-23T16:52:08.879640Z","shell.execute_reply.started":"2023-11-23T16:52:07.079701Z","shell.execute_reply":"2023-11-23T16:52:08.878401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#MinMax: SibSp, Parch, Fare\n#StdSc:  Age, Individual_Fare, Name_Score_Max, Name_Score_Mean, Name_Score_Sum, Name_Score_Max","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:52:08.881452Z","iopub.execute_input":"2023-11-23T16:52:08.881891Z","iopub.status.idle":"2023-11-23T16:52:08.886464Z","shell.execute_reply.started":"2023-11-23T16:52:08.881854Z","shell.execute_reply":"2023-11-23T16:52:08.885413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ***Split, Encoding and Feature Scaling:***","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nx_data_scaled = pd.get_dummies(x_data)\n \n_minmax = ['SibSp', 'Parch', 'Fare']\nscaler_1 = MinMaxScaler()\nx_data_scaled[_minmax] = pd.DataFrame( scaler_1.fit_transform(x_data_scaled[_minmax]), columns=_minmax)\n\n_stdsc =['Age', 'Individual_Fare', 'Name_Score_Max', 'Name_Score_Mean', 'Name_Score_Sum']\nscaler_2 = StandardScaler()\nx_data_scaled[_stdsc] = pd.DataFrame( scaler_2.fit_transform(x_data_scaled[_stdsc]), columns=_stdsc)\n\nx_data_scaled","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:52:08.887970Z","iopub.execute_input":"2023-11-23T16:52:08.888484Z","iopub.status.idle":"2023-11-23T16:52:09.129797Z","shell.execute_reply.started":"2023-11-23T16:52:08.888445Z","shell.execute_reply":"2023-11-23T16:52:09.128796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On last line(Survived) we observe the correlation with other features. Embarked_Q is the most uncorrelated with our label Survived.","metadata":{}},{"cell_type":"code","source":"x_data_scaled = x_data_scaled.drop(['Embarked_Q','Survived','Name_Score_Sum','Name_Score_Max'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:52:09.131302Z","iopub.execute_input":"2023-11-23T16:52:09.132228Z","iopub.status.idle":"2023-11-23T16:52:09.138986Z","shell.execute_reply.started":"2023-11-23T16:52:09.132189Z","shell.execute_reply":"2023-11-23T16:52:09.137731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_submission = x_data_scaled.loc[891:]\nx_datatrain = x_data_scaled.loc[y_data.index,:]","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:52:09.142561Z","iopub.execute_input":"2023-11-23T16:52:09.143491Z","iopub.status.idle":"2023-11-23T16:52:09.151964Z","shell.execute_reply.started":"2023-11-23T16:52:09.143450Z","shell.execute_reply":"2023-11-23T16:52:09.150761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ***Model Selection:***","metadata":{}},{"cell_type":"code","source":"!pip install pycaret","metadata":{"execution":{"iopub.status.busy":"2023-11-22T19:26:14.307485Z","iopub.execute_input":"2023-11-22T19:26:14.307858Z","iopub.status.idle":"2023-11-22T19:26:31.522187Z","shell.execute_reply.started":"2023-11-22T19:26:14.307788Z","shell.execute_reply":"2023-11-22T19:26:31.520443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pycaret.classification import setup, compare_models\n\npycaret_data = pd.concat([ x_datatrain, y_data], axis=1)\n_ = setup( data=pycaret_data , target='Survived', session_id=0, train_size=0.7, fold=20, fold_shuffle=True, preprocess=False, verbose=1)\nmodels_set = compare_models(n_select=5)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T19:35:03.310855Z","iopub.execute_input":"2023-11-22T19:35:03.311351Z","iopub.status.idle":"2023-11-22T19:36:13.893287Z","shell.execute_reply.started":"2023-11-22T19:35:03.311309Z","shell.execute_reply":"2023-11-22T19:36:13.892033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### ***Models based on 5 top scored of pycaret selection***","metadata":{}},{"cell_type":"code","source":"for i,model in enumerate(models_set):\n    m = str(model.__class__).split('.')[-1][:-2]\n    try: model.fit(x_datatrain, y_data, verbose=0)\n    except: model.fit(x_datatrain, y_data)\n        \n    y_pred = model.predict(x_submission)\n    submission = pd.DataFrame(y_pred, columns=['Survived'], index=range(892,1310))\n    submission.index.name = 'PassengerId'\n    submission.to_csv(f'model_{m}.csv')\n    print(f'{i+1} - Model {m} saved!')","metadata":{"execution":{"iopub.status.busy":"2023-11-22T19:36:27.677110Z","iopub.execute_input":"2023-11-22T19:36:27.677626Z","iopub.status.idle":"2023-11-22T19:36:30.760761Z","shell.execute_reply.started":"2023-11-22T19:36:27.677576Z","shell.execute_reply":"2023-11-22T19:36:30.759618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"#### ***Model based on Bagging***","metadata":{}},{"cell_type":"code","source":"bagging = {}\n\nfor model in models_set:\n    model_name = str(model.__class__).split('.')[-1][:-2]\n    bagging[model_name] = model.predict(x_submission)\n\nbagging = pd.DataFrame(bagging)\n\nweight = [1,1,1,1,1]\ntotal = sum(weight)\n\nBag = bagging.apply(lambda x: round(sum(map(lambda a,b: a*b, x, weight))/total), axis=1 )\n\nsubmission = pd.DataFrame(Bag.values, columns=['Survived'], index=range(892,1310))\nsubmission.index.name = 'PassengerId'\nsubmission.to_csv('bag_of_5models.csv')","metadata":{"execution":{"iopub.status.busy":"2023-11-22T19:30:08.048462Z","iopub.execute_input":"2023-11-22T19:30:08.049283Z","iopub.status.idle":"2023-11-22T19:30:08.214800Z","shell.execute_reply.started":"2023-11-22T19:30:08.049241Z","shell.execute_reply":"2023-11-22T19:30:08.213626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n-------","metadata":{}},{"cell_type":"markdown","source":"#### ***Model based on Neural Networks***","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras.layers as ly\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n\nfeats = x_datatrain.shape[1]\n\nlayers = [ ly.Input(shape=( feats,)) ]\nlayers += [ ly.Dense(feats, activation='tanh'), ly.Dense(1, activation='sigmoid')]\n#layers += [ ly.Dense(feats, activation='tanh') for _ in range(20)] + [ly.Dense(1, activation='sigmoid')]\n\nmodel = tf.keras.Sequential(layers)\n#model.summary()\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n                  loss=tf.keras.losses.BinaryCrossentropy(), \n                  metrics=['accuracy'])\n\nx = tf.constant(x_datatrain, dtype=tf.float32)\ny = tf.constant(y_data, dtype=tf.float32)\n\nhistory = model.fit( x, y, epochs=600, verbose=0, validation_split=0.2, callbacks=[\n                        ReduceLROnPlateau(monitor='accuracy', patience=3, factor=0.25, verbose=1),\n                        EarlyStopping(monitor='accuracy', patience=5, restore_best_weights=True)\n                    ])\n\nprint('\\n'), pd.DataFrame(history.history).plot()\nprint('\\n'), model.evaluate(x,y)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:55:15.723002Z","iopub.execute_input":"2023-11-23T16:55:15.723836Z","iopub.status.idle":"2023-11-23T16:55:35.442025Z","shell.execute_reply.started":"2023-11-23T16:55:15.723795Z","shell.execute_reply":"2023-11-23T16:55:35.440077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame(np.round(model.predict(tf.constant(x_submission, dtype=tf.float32), verbose=0)), columns=['Survived'], index=range(892,1310))\nsubmission.index.name = 'PassengerId'\nsubmission.Survived = submission.Survived.astype(int)\nsubmission.to_csv('model_ann_2.csv')","metadata":{"execution":{"iopub.status.busy":"2023-11-23T17:00:37.723211Z","iopub.execute_input":"2023-11-23T17:00:37.723653Z","iopub.status.idle":"2023-11-23T17:00:37.821622Z","shell.execute_reply.started":"2023-11-23T17:00:37.723622Z","shell.execute_reply":"2023-11-23T17:00:37.820523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"___","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}